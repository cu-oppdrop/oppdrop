name: Scrape Opportunities

on:
  schedule:
    - cron: '0 6 */3 * *'  # Every 3 days at 6am UTC
  workflow_dispatch:      # Manual trigger

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install requests beautifulsoup4

      # URF scraper requires cookies - only run if secret exists
      - name: Create URF cookies file
        env:
          URF_COOKIES: ${{ secrets.URF_COOKIES }}
        if: env.URF_COOKIES != ''
        run: echo '${{ secrets.URF_COOKIES }}' > scrapers/cookies.json

      - name: Run URF scraper
        if: hashFiles('scrapers/cookies.json') != ''
        run: python scrapers/urf_scraper.py

      # MEI scraper (public, no auth needed)
      - name: Run MEI scraper
        run: python scrapers/mei_scraper.py

      # Apply manual overrides (preserves manual edits)
      - name: Apply overrides
        run: python scrapers/apply_overrides.py

      - name: Copy data to docs
        run: cp data/opportunities.json docs/

      - name: Commit changes
        run: |
          git config user.email "action@github.com"
          git config user.name "GitHub Action"
          git add data/ docs/
          git diff --staged --quiet || git commit -m "Update opportunities $(date -u +%Y-%m-%d)"
          git push
